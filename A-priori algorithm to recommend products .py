# -*- coding: utf-8 -*-
"""Final-0122-CS246 - Homework 1-2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sOrG89qHY3f4_zJ57QqYjGFNGPj_pLwM

# CS246 - Homework 1-2

### Setup
"""



"""Let's setup Spark on your Colab environment.  Run the cell below!"""

!pip install pyspark
!pip install -U -q PyDrive
!apt install openjdk-8-jdk-headless -qq
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"

"""Now we authenticate a Google Drive client to download the file we will be processing in our Spark job.

**Make sure to follow the interactive instructions.**
"""

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# Authenticate and create the PyDrive client
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

"""### Your task"""

from pyspark.sql import *
from pyspark.sql.functions import *
from pyspark import SparkContext
import pandas as pd

# create the Spark Session
spark = SparkSession.builder.getOrCreate()
  
# create the Spark Context
sc = spark.sparkContext

rdd = sc.textFile("browsing.txt")
baskets = rdd.map(lambda l: l.split())
baskets = baskets.map(lambda b: sorted(set(b)))

#for single item 
#append each item 1 in tuple ('EFFF') --> ('EFFF',1)
def singleitem(basket):
    item = []
    for i in basket:
        item.append((i,1))
    return item
single_itemset = baskets.flatMap(singleitem)

#Cacluate the frequent single item and filter the number less than 100
single_item_total = single_itemset.reduceByKey(lambda x, y: x + y) # ((a b) num)
single_item_freq = single_item_total.filter(lambda kv: kv[1] >= 100) # filter connected
single_item_freq_list = single_item_freq.collect()

#Make a list that only contains ['EFF'] for the frequent single item
first_element= single_item_freq.map(lambda x: ((x[0])))
first_element.take(5)
first_element_list = first_element.collect()

#for pairs
# Create the pairs that the pairs of item is the frequent item calculated previously
# Use the first_element to check if the docuement going to make the pairs that the item is belong to the frequent basket
def pairs(l):
    result = []
    for p1 in range(len(l)):
      if l[p1] in first_element_list:
        for p2 in range(p1+1,len(l)):
          if l[p2] in first_element_list:
            result.append(((l[p1],l[p2]),1))
    return result
pairs_in_frequent_item = baskets.flatMap(pairs)
pairs_in_frequent_item.count()

# Count the pairs and filter out the count less than 100
pairs_in_frequent_item_count = pairs_in_frequent_item.reduceByKey(lambda x, y: x + y) # ((a b) num)
pairs_in_frequent_item_count = pairs_in_frequent_item_count.filter(lambda kv: kv[1] >= 100) # filter connected
pairs_in_frequent_item_count.count()

pairs_in_frequent_item_count_list = pairs_in_frequent_item_count.collect()

# Found the value count for the frequency pairs
keys_freqpair = [] 
values_freqpair = [] 
for item in pairs_in_frequent_item_count_list: 
    keys_freqpair.append(item[0]), values_freqpair.append(item[1])

# Found the index for the frequent pair (item1,item2), that the item1 index in the original frequenct single item list
index2 = []
for p2 in range(len(pairs_in_frequent_item_count_list)):
    for p1 in range(len(first_element_list)):
      if pairs_in_frequent_item_count_list[p2][0][0] in first_element_list[p1]:
        index2.append(first_element_list.index(pairs_in_frequent_item_count_list[p2][0][0]))

#Calculate the confidence for (item1,item2) that is item 1 --> item 2
item1to2 = []
for i in range(len(pairs_in_frequent_item_count_list)):
    index = index2[i]
    uv_conf = values_freqpair[i]/single_item_freq_list[index][1]
    item1to2.append((pairs_in_frequent_item_count_list[i][0], uv_conf))

item1to2 = sc.parallelize(item1to2)
doubles_conf_item1to2 = item1to2.sortBy(lambda x: (-x[1], x[0]))
doubles_conf_item1to2.take(5)

# Found the index for the frequent pair (item1,item2), that the item2 index in the original frequenct single item list
index3 = []
for p2 in range(len(pairs_in_frequent_item_count_list)):
    for p1 in range(len(first_element_list)):
      if pairs_in_frequent_item_count_list[p2][0][0] in first_element_list[p1]:
        index3.append(first_element_list.index(pairs_in_frequent_item_count_list[p2][0][1]))

#Calculate the confidence for (item1,item2) that is item 2 --> item 1
item2to1 = []
for i in range(len(pairs_in_frequent_item_count_list)):
    index = index3[i]
    uv_conf = values_freqpair[i]/single_item_freq_list[index][1]
    item2to1.append((pairs_in_frequent_item_count_list[i][0], uv_conf))

item2to1 = sc.parallelize(item2to1)
doubles_conf_item2to1 = item2to1.sortBy(lambda x: (-x[1], x[0]))
doubles_conf_item2to1.take(5)

# for triple
# find triple frequent item 
pairs_first= pairs_in_frequent_item_count.map(lambda x: ((x[0])))
pairs_first_list = pairs_first.collect()

def triple_t(l):
    triple_t = []
    for p1 in range(len(l)):
        for p2 in range(p1):
           for p3 in range(p2):
            if (l[p1],l[p2]) and (l[p2],l[p3]) and (l[p1],l[p3]) in pairs_first_list:
              if l[p1]!=l[p2]!=l[p3]:
                triple_t.append(((l[p1],l[p2],l[p3]),1))
    return triple_t

triple_test = baskets.flatMap(triple_t)

t= triple_test.reduceByKey(lambda x, y: x + y) # ((a b) num)
t = t.filter(lambda kv: kv[1] >= 100)

# find triple frequent item - index
s = {}
for item, support in single_item_freq.collect():
    s[item] = support
s = sc.broadcast(s)
p = {}
for entry, support in pairs_in_frequent_item.collect():
    p[entry] = support
p = sc.broadcast(p)

def confidence_triples_helper(t):
    doubles = p.value
    triple, support = t
    support = float(support)
    u, v, s = triple
    uv_s = support / doubles[u, v]
    us_v = support / doubles[u, s]
    vs_u = support / doubles[v, s]
    return (('(%s, %s) -> %s' % (u, v, s), uv_s),
            ('(%s, %s) -> %s' % (u, s, v), us_v),
            ('(%s, %s) -> %s' % (v, s, u), vs_u))

triples_conf = triple_test.flatMap(confidence_triples_helper)
triples_conf = triples_conf.sortBy(lambda x: (-x[1], x[0]))