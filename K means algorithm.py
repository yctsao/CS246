# -*- coding: utf-8 -*-
"""Copy of 0206-spark-CS246 - Homework 2-2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zAWqDW1nwpBtB4Dho1ERl7vRxnMMTypc

# CS246 - Homework 2-2

### Setup

Let's setup Spark on your Colab environment.  Run the cell below!
"""

!pip install pyspark
!pip install -U -q PyDrive
!apt install openjdk-8-jdk-headless -qq
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"

"""Now we authenticate a Google Drive client to download the file we will be processing in our Spark job.

**Make sure to follow the interactive instructions.**
"""

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# Authenticate and create the PyDrive client
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

id='1E-voIV2ctU4Brw022Na8RHVVRGOoNkO1'
downloaded = drive.CreateFile({'id': id})
downloaded.GetContentFile('data.txt')

id='1yXNlZWMqUcAwDScBrkFChOHJwR1FZXmI'
downloaded = drive.CreateFile({'id': id})
downloaded.GetContentFile('c1.txt')

id='1vfovle9DgaeK0LnbQTH0j7kRaJjsvLtb'
downloaded = drive.CreateFile({'id': id})
downloaded.GetContentFile('c2.txt')

"""### Your task"""

from pyspark.sql import *
import pyspark.sql.functions as F
from pyspark import SparkContext
import pandas as pd
import numpy as np


# create the Spark Session
spark = SparkSession.builder.getOrCreate()
  
# create the Spark Context
sc = spark.sparkContext

def parseVector(line):
    return np.array([float(x) for x in line.split(' ')])


def closestPoint(p, centers):
    bestIndex = 0
    closest = float("+inf")
    for i in range(len(centers)):
        tempDist = np.sqrt(np.sum((p - centers[i]) ** 2))
        if tempDist < closest:
            closest = tempDist
            bestIndex = i
    return bestIndex

lines = spark.read.text("data.txt").rdd.map(lambda r: r[0])
data = lines.map(parseVector).cache()

lines_c1 = spark.read.text("c1.txt").rdd.map(lambda r: r[0])
c1 = lines.map(parseVector).cache()

lines_c2 = spark.read.text("c2.txt").rdd.map(lambda r: r[0])
c2 = lines.map(parseVector).cache()

K = 10
kPoints = c1.takeSample(False, K, 1)   #Randomly select 10 points
tempDist = 1.0

K = 10
kPoints = c2.takeSample(False, K, 1)   #Randomly select 10 points
tempDist = 1.0

for j in range (1,21):
  closest = data.map(lambda p: (closestPoint(p, kPoints), (p, 1)))
  pointStats = closest.reduceByKey(lambda p1_c1, p2_c2: (p1_c1[0] + p2_c2[0], p1_c1[1] + p2_c2[1]))
  newPoints = pointStats.map(lambda st: (st[0], st[1][0] / st[1][1])).collect()
  tempDist = (np.sum((kPoints[iK] - p) ** 2) for (iK, p) in newPoints)
  tempDist = list(tempDist)
  for (iK, p) in newPoints:
    kPoints[iK] = p
  #print("Final centers: " + str(kPoints))